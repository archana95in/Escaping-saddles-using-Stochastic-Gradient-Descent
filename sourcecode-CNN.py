# -*- coding: utf-8 -*-
"""Renaissance_CS6316.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GAj4cfW0fP43kR8xy7N7w6S8cfC4IYh5

# ESCAPING SADDLES USING STOCHASTIC GRADIENT DESCENT
http://proceedings.mlr.press/v80/daneshmand18a.html
## Authors: Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, Thomas Hofmann

### Reproduced by Renaissance (Archana, Kamya, Daniel, Yuancheng)

---
"""

# load necessary modules
import keras 
from keras import backend as K
from keras.datasets import mnist
from keras.layers import Dense, Flatten, Dropout
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import GaussianNoise
from keras.legacy import interfaces
from keras.models import Sequential 
from keras.optimizers import Optimizer 

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

"""##A. Initialization of Parameters"""

learning_rate = 0.001
num_epochs = 20
display_step = 500
num_classes = 10

"""##B. Custom Optimizers for CNC-PGD and ISO-PGD

### CNC PGD (Perturbed GD): 
The Gradient Descent under the CNC assumption (perturbed with SGD steps) performs better than GD perturbed with isotropic noise.
"""

class CNC_PGD_Optimizer(Optimizer):
    def __init__(self, learning_rate=1.0, rho=0.95, momentum=0, **kwargs):
        self.initial_decay = kwargs.pop('decay', 0.0)
        self.epsilon = kwargs.pop('epsilon', K.epsilon())
        learning_rate = kwargs.pop('lr', learning_rate)
        super(CNC_PGD_Optimizer, self).__init__(**kwargs)
        with K.name_scope(self.__class__.__name__):
            self.learning_rate = K.variable(learning_rate, name='learning_rate')
            self.decay = K.variable(self.initial_decay, name='decay')
            self.iterations = K.variable(0, dtype='int64', name='iterations')
            self.momentum = K.variable(momentum, name='momentum')
        self.rho = rho

    @interfaces.legacy_get_updates_support
    def get_updates(self, loss, params):
        grads = self.get_gradients(loss, params)
        shapes = [K.int_shape(p) for p in params]
        accumulators = [K.zeros(shape, name='accumulator_' + str(i))
                        for (i, shape) in enumerate(shapes)]
        delta_accumulators = [K.zeros(shape, name='delta_accumulator_' + str(i))
                              for (i, shape) in enumerate(shapes)]
        self.weights = [self.iterations] + accumulators + delta_accumulators
        self.updates = [K.update_add(self.iterations, 1)]

        lr = self.learning_rate
        if self.initial_decay > 0:
            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,
                                                      K.dtype(self.decay))))
            
        shapes = [K.int_shape(p) for p in params]
        moments = [K.zeros(shape, name='moment_' + str(i)) for (i, shape) in enumerate(shapes)]

        for p, g, a, d_a, m in zip(params, grads, accumulators, delta_accumulators, moments):
            v = self.momentum * m - lr * g
            # update accumulator
            new_a = self.rho * a + (1. - self.rho) * K.square(g)
            self.updates.append(K.update(a, new_a))

            # use the new accumulator and the *old* delta_accumulator
            update = g * K.sqrt(d_a + self.epsilon) / K.sqrt(new_a + self.epsilon)
            new_p = p - lr * update + v # Add Stochastic Gradient Step Here?

            # Apply constraints.
            if getattr(p, 'constraint', None) is not None:
                new_p = p.constraint(new_p)

            self.updates.append(K.update(p, new_p))

            # update delta_accumulator
            new_d_a = self.rho * d_a + (1 - self.rho) * K.square(update)
            self.updates.append(K.update(d_a, new_d_a))
        return self.updates

    def set_weights(self, weights):
        params = self.weights
        # Override set_weights for backward compatibility of Keras 2.2.4 optimizer
        # since it does not include iteration at head of the weight list. Set
        # iteration to 0.
        if len(params) == len(weights) + 1:
            weights = [np.array(0)] + weights
        super(Adadelta, self).set_weights(weights)

    def get_config(self):
        config = {'learning_rate': float(K.get_value(self.learning_rate)),
                  'rho': self.rho,
                  'decay': float(K.get_value(self.decay)),
                  'epsilon': self.epsilon}
        base_config = super(Adadelta, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

"""###ISO PGD (Perturbed GD): 
The Gradient Descent under the CNC assumption (perturbed with noise) performs better than GD
"""

class ISO_PGD_Optimizer(Optimizer):

    # 1. Take out Nesterov support from originial SGD code

    def __init__(self, learning_rate=0.01, momentum=0., **kwargs):
        learning_rate = kwargs.pop('lr', learning_rate)
        self.initial_decay = kwargs.pop('decay', 0.0)
        super(ISO_PGD_Optimizer, self).__init__(**kwargs)
        with K.name_scope(self.__class__.__name__):
            self.iterations = K.variable(0, dtype='int64', name='iterations')
            self.learning_rate = K.variable(learning_rate, name='learning_rate')
            self.momentum = K.variable(momentum, name='momentum')
            self.decay = K.variable(self.initial_decay, name='decay')

    @interfaces.legacy_get_updates_support
    def get_updates(self, loss, params):
        grads = self.get_gradients(loss, params)
        self.updates = [K.update_add(self.iterations, 1)]

        lr = self.learning_rate
        if self.initial_decay > 0:
            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,K.dtype(self.decay))))
        # momentum
        shapes = [K.int_shape(p) for p in params]
        moments = [K.zeros(shape, name='moment_' + str(i))
                   for (i, shape) in enumerate(shapes)]
        self.weights = [self.iterations] + moments
        for p, g, m in zip(params, grads, moments):
            v = self.momentum * m - lr * g  # velocity
            self.updates.append(K.update(m, v))

            new_p = p + v

            # Apply constraints.
            if getattr(p, 'constraint', None) is not None:
                new_p = p.constraint(new_p)

            self.updates.append(K.update(p, new_p))
        return self.updates

    def get_config(self):
        config = {'learning_rate': float(K.get_value(self.learning_rate)),
                  'momentum': float(K.get_value(self.momentum)),
                  'decay': float(K.get_value(self.decay)),
                  'nesterov': self.nesterov}
        base_config = super(SGD, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

"""##C. Training Models"""

# Find thetas using gradient descent
def GD(xtrain, ytrain, xtest, ytest):
    
    gd_loss = np.empty(num_epochs)
    xtrain = xtrain.reshape(-1, 28, 28, 1)
    xtest = xtest.reshape(-1, 28, 28, 1)
    
    model = Sequential()
    xtrain = xtrain.astype('float32')
    xtest = xtest.astype('float32')
    xtrain /= 255
    xtest /= 255

    # Adadelta is gradient descent optimizer for cnn

    ytrain = keras.utils.to_categorical(ytrain, num_classes)
    ytest = keras.utils.to_categorical(ytest, num_classes)

    model.add(Conv2D(filters=10, activation= 'relu', kernel_size=(5,5), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
  
    model.add(Conv2D(filters=8, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
  
    model.add(Conv2D(filters=5, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
  
    model.add(Conv2D(filters=5, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
  
    model.add(Flatten(input_shape=(28,28)))
    model.add(Dense(10, activation = 'softmax'))

    optimizer = keras.optimizers.Adadelta(lr=0.001, rho=0.95)    

    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    history = model.fit(xtrain, ytrain, epochs = num_epochs)

    return history

"""### CNC-SGD
SGD without perturbations, escapes the saddle point faster due to the intrinsic noise generated in each iteration due to sampling. We were unable to gain data regarding the CNC or Isotropic noise, thus used the SGD optimizer from the Keras Library
"""

def SGD(xtrain, ytrain, xtest, ytest):
    xtrain = xtrain.reshape(-1, 28, 28, 1)
    xtest = xtest.reshape(-1, 28, 28, 1)
    model = Sequential()
    xtrain = xtrain.astype('float32')
    xtest = xtest.astype('float32')
    xtrain /= 255
    xtest /= 255

    ytrain = keras.utils.to_categorical(ytrain, num_classes)
    ytest = keras.utils.to_categorical(ytest, num_classes)

    model.add(Conv2D(filters=10, activation= 'relu', kernel_size=(5,5), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
  
    model.add(Conv2D(filters=8, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
  
    model.add(Conv2D(filters=5, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
  
    model.add(Conv2D(filters=5, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
  
    model.add(Flatten(input_shape=(28,28)))
    model.add(Dense(10, activation = 'softmax'))

    optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=0.1, nesterov=True)

    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    history = model.fit(xtrain, ytrain, epochs = num_epochs)

    return history

def ISO_PGD(xtrain, ytrain, xtest, ytest):
    gd_loss = np.empty(num_epochs)
    xtrain = xtrain.reshape(-1, 28, 28, 1)
    xtest = xtest.reshape(-1, 28, 28, 1)
    
    model = Sequential()
    xtrain = xtrain.astype('float32')
    xtest = xtest.astype('float32')
    
    xtrain /= 255
    xtest /= 255
    model.add(GaussianNoise(0.01))

    ytrain = keras.utils.to_categorical(ytrain, num_classes)
    ytest = keras.utils.to_categorical(ytest, num_classes)

    model.add(Conv2D(filters=10, activation= 'relu', kernel_size=(5,5), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
    model.add(GaussianNoise(0.1))
    model.add(Conv2D(filters=8, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))

    model.add(Conv2D(filters=5, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))

    model.add(Conv2D(filters=5, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
  
    model.add(Flatten(input_shape=(28,28)))
    model.add(Dense(10, activation = 'softmax'))

    optimizer = ISO_PGD_Optimizer(lr=0.001)    

    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    history = model.fit(xtrain, ytrain, epochs = num_epochs)

    return history

def CNC_PGD (xtrain, ytrain, xtest, ytest):
    
    gd_loss = np.empty(num_epochs)
    xtrain = xtrain.reshape(-1, 28, 28, 1)
    xtest = xtest.reshape(-1, 28, 28, 1)
    
    model = Sequential()
    xtrain = xtrain.astype('float32')
    xtest = xtest.astype('float32')
    xtrain /= 255
    xtest /= 255
        
    ytrain = keras.utils.to_categorical(ytrain, num_classes)
    ytest = keras.utils.to_categorical(ytest, num_classes)
    
    model.add(Conv2D(filters=10, activation= 'relu', kernel_size=(5,5), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
    
    model.add(Conv2D(filters=8, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
    
    model.add(Conv2D(filters=5, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
    
    model.add(Conv2D(filters=5, activation= 'relu', kernel_size=(3,3), strides=1))
    model.add(MaxPooling2D(pool_size=(2,2), strides=1))
    
    model.add(Flatten(input_shape=(28,28)))
    model.add(Dense(10, activation = 'softmax'))
    
    optimizer = CNC_PGD_Optimizer(lr=0.001, rho=0.95)    
    
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    
    history = model.fit(xtrain, ytrain, epochs = num_epochs)
        
    return history

"""##D. Plotting and Comparison"""

def plot_multiple_history(h1, h2, h3, h4):

    loss_1 = h1.history['loss']
    acc_1 = h1.history['acc']
    loss_2 = h2.history['loss']
    acc_2 = h2.history['acc']
    loss_3 = h3.history['loss']
    acc_3 = h3.history['acc']
    loss_4 = h4.history['loss']
    acc_4 = h4.history['acc']

    epoch = [i for i in range(1, num_epochs+1)]

    plt.figure(figsize = (20, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epoch, loss_1, label = 'GD', color='green')
    plt.plot(epoch, loss_2, label = 'SGD', color='blue')
    plt.plot(epoch, loss_3, label = 'CNC_PGD', color='black')
    plt.plot(epoch, loss_4, label = 'ISO_PGD', color='red')
    plt.ylabel("loss")
    plt.xlabel("Epoch")
    plt.legend(loc = 'best')
    plt.show()

if __name__ == '__main__':
  (xtrain, ytrain), (xtest, ytest) = mnist.load_data()
  
  h2 = SGD(xtrain, ytrain, xtest, ytest)
  h4 = ISO_PGD(xtrain, ytrain, xtest, ytest)
  h3 = CNC_PGD(xtrain, ytrain, xtest, ytest)
  h1 = GD(xtrain, ytrain, xtest, ytest)

plot_multiple_history(h1, h2, h3, h4)

"""**## Optionally add more sections**
# II. Additional Intuitions: Why do submodular functions work?
"""